
<!-- saved from url=(0035)https://kgbmax.github.io/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script async="" src="./FERV39k/analytics.js"></script><script src="./FERV39k/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:16px;
		margin-left: auto;
		margin-right: auto;
		width: 800px;
	}
	
	h1 {
		font-weight:300;
	}
		
	h2 {
		font-weight:300;
		font-size: 22px;
		text-align: left;
	}

	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}

	pre {
    text-align: left;
    white-space: pre;
	background-color: ghostwhite;
	border: 1px solid #CCCCCC;
	padding: 10px 20px;
	margin: 10px;
    tab-size:         4; /* Chrome 21+, Safari 6.1+, Opera 15+ */
    -moz-tab-size:    4; /* Firefox 4+ */
    -o-tab-size:      4; /* Opera 11.5 & 12.1 only */
  	}

</style>


  
		<title>FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition
in Videos</title>
		<meta property="og:image" content="">
		<meta property="og:title" content="FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition
in Videos">
</head>

<body>
<br>
<center>
<span style="font-size:40px">FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition
in Videos</span>
<table align="center" width="750px">
<tbody><tr>
<td align="center" width="80px">
<center>
<span style="font-size:24px"><a href="https://wangyanckxx.github.io/">Yan Wang</a></span>
</center>
</td>
<td align="center" width="100px">
<center>
<span style="font-size:24px"><a href="http://www.fudanroilab.com/2019/10/07/YixuanSun.html">Yixuan Sun</a></span>
</center>
</td>
<td align="center" width="100px">
<center>
<span style="font-size:24px"><a href="https://space.bilibili.com/401742377">Yiwen Huang</a></span>
</center>
</td>
<td align="center" width="100px">
<center>
<span style="font-size:24px"><a href="http://www.fudanroilab.com/2019/01/17/ZhongyingLiu.html">Zhongying Liu</a></span>
</center>
</td>
<td align="center" width="100px">
<center>
<span style="font-size:24px"><a href="http://www.fudanroilab.com/2020/07/01/ShuyongGao.html">Shuyong Gao</a></span>
</center>
</td>

</td>
</tr></tbody></table>
<table align="center" width="800px">
<tbody><tr>
<td align="center" width="100px">
<center>
<span style="font-size:24px"><a href="http://www.weifengge.net/">Weifeng Ge</a></span>
</center>
</td>
<td align="center" width="100px">
<center>
<span style="font-size:24px"><a href="http://faet.fudan.edu.cn/17/bb/c13532a137147/page.htm">Wenqiang Zhang</a></span>
</center>
</td>
<td align="center" width="100px">
<center>
<span style="font-size:24px"><a href="https://faculty.fudan.edu.cn/zhangwei1234/zh_CN/jsxx/161831/jsxx/jsxx.htm">Wei Zhang</a></span>
</center>
</td>
</tr></tbody></table>
<!-- <span style="font-size:30px">ECCV 2016.</span> -->

			  <table align="center" width="600px">
				  <tbody><tr>
					  <td align="center" width="100px">
						<center>
							<span style="font-size:20px"><a href="https://www.fudan.edu.cn/">Fudan University</a></span>
						</center>
					  </td>
			  </tr></tbody></table>
			IEEE Conference on Computer Vision and Pattern Recognition (<a href="http://cvpr2022.thecvf.com/" target="_blank">CVPR</a>) 2022<!--, <font color="#e86e14">Oral Presentation</font>-->
          </center>

   		  <br><br>
		  <hr>

  		  <br>
  		  <table align="center" width="720px">
  			  <tbody><tr>
  	              <td width="400px">
  					<center>
  	                	<a href="./FERV39k/teaser.png"><img class="rounded" src="./FERV39k/teaser.png" width="800px"></a><br>
					</center>
  	              </td>
                </tr>
  	              <tr><td width="400px">
  					<center>
  	                	<span style="font-size:14px"><i>An overview of FERV39k composed by video frames of 7 basic expression across 4 scenarios subdivided by 22 scenes.</i>
					</span></center>
  	              </td>

  		  </tr></tbody></table>
      	  <br><br>

		  <table align="center" width="720px">
			<!-- <center><h1>Download</h1></center> -->
			<tbody><tr>
				<td width="300px">
					<center>
						<a href="https://kgbmax.github.io/#download"><img src="./FERV39k/data_icon.png" height="120px"></a><br>
						<span style="font-size:16px">Download (annotations)</span><br>
					</center>
				</td>
				<td width="300px">
					<center>
						<a href="https://kgbmax.github.io/#video"><img src="./FERV39k/video_icon.png" height="120px"></a><br>
						<span style="font-size:16px">Videos at CVPR'2022</span><br>
					</center>
				</td>
				<td width="300px">
					<center>
						<a href="https://kgbmax.github.io/#analysis"><img src="./FERV39k/magnify_glass.png" height="120px"></a><br>
						<span style="font-size:16px">Analysis</span><br>
					</center>
				</td>
				<td width="300px">
					<center>
					  <a href="https://github.com/KGBMAX/FERV39K"><img src="./FERV39k/github_icon.png" height="120px"></a><br>
					  <span style="font-size:16px">GitHub Repo</span><br>						
					</center>
				</td>
			</tr>
		  </tbody></table>

		  <br><br>

		  <hr>

  		  <center><h1>Abstract</h1></center><table align="center" width="720px">
				
		  </table>
	      <p style="text-align:justify; text-justify:inter-ideograph;">
			<span>

			&nbsp&nbsp&nbsp&nbspCurrent benchmarks for facial expression recognition (FER) mainly focus on static images,
			while there are limited datasets for FER in videos. It is still ambiguous to evaluate
			whether performances of existing methods remain satisfactory in real-world application-oriented
			scenes. For example, “Happy” expression with high intensity in Talk-Show is more discriminating
			than the same expression with low intensity in Official-Event. To fill this gap, we build a large-scale
			multi-scene dataset, coined as <b>FERV39k</b>. We first analyze the important ingredients of constructing such a novel
			dataset in three aspects: (1) multi-scene hierarchy and expression class,
			(2) generation of candidate video clips, (3) trusted manual labelling process. Based on these guidelines,
				we select <b>4 scenarios</b> subdivided into <b>22 scenes</b>, annotate <b>86k samples</b> automatically obtained from <b>4k videos</b>
			based on the well-designed workflow, and finally build <b>38,935 video clips</b> labeled with <b>7 classic expressions</b>.
			To benchmark it, we adopt and design four kinds of baseline frameworks for dynamic FER (DFER) and further
			give an in-depth analysis on their performance across different scenes and reveal some challenges
			for future research. To benchmark it, we adopt and design four kinds of baseline frameworks
			for dynamic FER (DFER) and further give an in-depth analysis on their performance across
			different scenes and reveal some challenges for future research. Besides, we systematically investigate
			key components of FER in videos by ablation studies. We hope the FERV39k can advance research towards DFER.
		  	</span>
		  </p>
  		  <br><br>
		  <hr>

		  <!-- <center><h1>Demo video</h1></center><table align="center" width="720px">
			
			<tbody><tr>
				</tr></tbody></table><table align="center" width="720px">
					<tbody><tr>
						<td align="center" width="720px">
							<iframe width="600" height="320" src="./FERV39k/notLDzBJ2mg.html" frameborder="0" allowfullscreen=""></iframe>
						</td>
					  </tr>
					<tr>
						<td align="center" width="720px">
						  <span style="font-size:14px"><i>
							An illustrative video of FineGym's hiecharcial annotations given a complete competition.
							Action and subaction boundaries are highlighted while irrelevant fragments are fast-forwarded.
							We also present the tree-based process at the end of the demo video.</i>
						</span>
						   </td>
					  </tr>
					 </tbody></table>
			  
		  
		   <br><br>
		  <hr>

		  <center><h1>5-Minute Oral presentation video</h1></center><table id="video" align="center" width="720px">
			
			<tbody><tr>
				</tr></tbody></table><table align="center" width="720px">
					<tbody><tr>
						<td align="center" width="720px">
							<iframe width="600" height="320" src="./FERV39k/nXpB5l40trg.html" frameborder="0" allowfullscreen=""></iframe>
						</td>
					  </tr>
					<tr>
					 </tr></tbody></table>
			  
		  
		   <br><br>
		  <hr>

		  <center><h1>1-Minute presentation video</h1></center><table align="center" width="720px">
			
			<tbody><tr>
				</tr></tbody></table><table align="center" width="720px">
					<tbody><tr>
						<td align="center" width="720px">
							<iframe width="600" height="320" src="./FERV39k/b1Sm1WDcypw.html" frameborder="0" allowfullscreen=""></iframe>
						</td>
					  </tr>
					<tr>
					 </tr></tbody></table>
			  
		  
		   <br><br>
		  <hr> -->


		  <center><h1>Dataset hierarchy</h1></center><table align="center" width="720px">
			
			<tbody><tr>
				<td width="400px">
				  <center>
					  <a><img class="rounded" src="./FERV39k/hierarchy.png" width="800px"></a><br>
				</center>
				</td>
			</tr>
				<tr><td width="400px">
				  <center>
					  <span style="font-size:14px"><i>
						 Our design of 4 isolated scenarios, 22 scenes, 7 basic expressions and 26 descriptors.
						</i>
				</span></center>
				</td>

		  </tr></tbody></table>
	      <br><br>
		  <hr>

<center><h1>Examples of scenes</h1></center><table align="center" width="720px">

<span>
&nbsp&nbsp&nbsp&nbsp
We present several examples of expressions in several scenes.
Each group belongs to seven basic express-ions in a scene (Happy, Sad, Surprise, Fear, Angry, Disgust, and Neutral).
It can be seen the impact of scenes to an expression.
</span>

<tbody><tr>
<td width="400px">
	<center>
		<a><img class="rounded" src="./FERV39k/GIF/dailylife1.gif" width="200px"></a>
		<a><img class="rounded" src="./FERV39k/GIF/dailylife2.gif" width="200px"></a>
		<a><img class="rounded" src="./FERV39k/GIF/dailylife3.gif" width="200px"></a>
		<br>
</center>
</td>
</tr>
<tr><td width="400px">
	<center>
		<span style="font-size:14px"><i>
			Dailylife
		</i>
</span></center>
</td>

<tbody><tr>
<td width="400px">
	<center>
		<a><img class="rounded" src="./FERV39k/GIF/ElegantArt1.gif" width="200px"></a>
		<a><img class="rounded" src="./FERV39k/GIF/ElegantArt2.gif" width="200px"></a>
		<a><img class="rounded" src="./FERV39k/GIF/ElegantArt3.gif" width="200px"></a>
		<br>
</center>
</td>
</tr>
<tr><td width="400px">
	<center>
		<span style="font-size:14px"><i>
			Elegant Art
		</i>
</span></center>
</td>

<tbody><tr>
<td width="400px">
	<center>
		<a><img class="rounded" src="./FERV39k/GIF/Medicine1.gif" width="200px"></a>
		<a><img class="rounded" src="./FERV39k/GIF/Medicine2.gif" width="200px"></a>
		<a><img class="rounded" src="./FERV39k/GIF/Medicine3.gif" width="200px"></a>
		<br>
</center>
</td>
</tr>
<tr><td width="400px">
	<center>
		<span style="font-size:14px"><i>
			Medicine
		</i>
</span></center>
</td>

<tbody><tr>
<td width="400px">
	<center>
		<a><img class="rounded" src="./FERV39k/GIF/school1.gif" width="200px"></a>
		<a><img class="rounded" src="./FERV39k/GIF/school2.gif" width="200px"></a>
		<a><img class="rounded" src="./FERV39k/GIF/school3.gif" width="200px"></a>
		<br>
</center>
</td>
</tr>
<tr><td width="400px">
	<center>
		<span style="font-size:14px"><i>
			School
		</i>
</span></center>
</td>

</tr></tbody></table>
<br><br>
<hr>
	

		  <!-- <center><h1>Examples of scenes</h1></center><p style="text-align:justify; text-justify:inter-ideograph;">
				<span>
					&nbsp&nbsp&nbsp&nbspWe present several examples of expressions in several scenes.
					Each group belongs to seven basic expressions in a scene (Happy, Sad, Surprise, Fear, Angry, Disgust, and Neutral).
					It can be seen the impact of scenes to an expression.</span>
				<br>
				</p><table align="center" width="720px">
			
			<tbody><tr>
				
				<td width="360px">
					<center>
						<span style="font-size:22px"><a>Balance Beam (BB)</a></span><br>
						<a><img onmouseover="this.src=&#39;./resources/examples/example_bb_01_slow.gif&#39;;" onmouseout="this.src=&#39;./resources/examples/example_bb_01_normal.gif&#39;;" src="./FERV39k/example_bb_01_normal.gif" width="100px"></a>
						<a><img onmouseover="this.src=&#39;./resources/examples/example_bb_02_slow.gif&#39;;" onmouseout="this.src=&#39;./resources/examples/example_bb_02_normal.gif&#39;;" src="./FERV39k/example_bb_02_normal.gif" width="100px"></a>
						<a><img onmouseover="this.src=&#39;./resources/examples/example_bb_03_slow.gif&#39;;" onmouseout="this.src=&#39;./resources/examples/example_bb_03_normal.gif&#39;;" src="./FERV39k/example_bb_03_normal.gif" width="100px"></a>
					</center>
				</td>
				<td width="360px">
					<center>
						<span style="font-size:22px"><a>Floor Exercise (FX)</a></span><br>
						<a><img onmouseover="this.src=&#39;./resources/examples/example_fx_01_slow.gif&#39;;" onmouseout="this.src=&#39;./resources/examples/example_fx_01_normal.gif&#39;;" src="./FERV39k/example_fx_01_normal.gif" width="100px"></a>
						<a><img onmouseover="this.src=&#39;./resources/examples/example_fx_02_slow.gif&#39;;" onmouseout="this.src=&#39;./resources/examples/example_fx_02_normal.gif&#39;;" src="./FERV39k/example_fx_02_normal.gif" width="100px"></a>
						<a><img onmouseover="this.src=&#39;./resources/examples/example_fx_03_slow.gif&#39;;" onmouseout="this.src=&#39;./resources/examples/example_fx_03_normal.gif&#39;;" src="./FERV39k/example_fx_03_normal.gif" width="100px"></a>
					</center>
				</td>
			</tr>
			<tr>
				<td width="360px">
					<center>
						<span style="font-size:22px"><a>Uneven Bar (UB)</a></span><br>
						<a><img onmouseover="this.src=&#39;./resources/examples/example_ub_01_slow.gif&#39;;" onmouseout="this.src=&#39;./resources/examples/example_ub_01_normal.gif&#39;;" src="./FERV39k/example_ub_01_normal.gif" width="100px"></a>
						<a><img onmouseover="this.src=&#39;./resources/examples/example_ub_02_slow.gif&#39;;" onmouseout="this.src=&#39;./resources/examples/example_ub_02_normal.gif&#39;;" src="./FERV39k/example_ub_02_normal.gif" width="100px"></a>
						<a><img onmouseover="this.src=&#39;./resources/examples/example_ub_03_slow.gif&#39;;" onmouseout="this.src=&#39;./resources/examples/example_ub_03_normal.gif&#39;;" src="./FERV39k/example_ub_03_normal.gif" width="100px"></a>
					</center>
				</td>
				<td width="360px">
					<center>
						<span style="font-size:22px"><a>Vault (VT)</a></span><br>
						<a><img onmouseover="this.src=&#39;./resources/examples/example_vt_01_slow.gif&#39;;" onmouseout="this.src=&#39;./resources/examples/example_vt_01_normal.gif&#39;;" src="./FERV39k/example_vt_01_normal.gif" width="100px"></a>
						<a><img onmouseover="this.src=&#39;./resources/examples/example_vt_02_slow.gif&#39;;" onmouseout="this.src=&#39;./resources/examples/example_vt_02_normal.gif&#39;;" src="./FERV39k/example_vt_02_normal.gif" width="100px"></a>
						<a><img onmouseover="this.src=&#39;./resources/examples/example_vt_03_slow.gif&#39;;" onmouseout="this.src=&#39;./resources/examples/example_vt_03_normal.gif&#39;;" src="./FERV39k/example_vt_03_normal.gif" width="100px"></a>
					</center>
				</td>
			</tr>
		  </tbody></table>

		  <br><br>
		  <hr> -->

		  <center><h1>Empirical Studies and Analysis</h1></center><table id="analysis" align="center" width="720px">
			
			</table><center><h2> (1) Results of four kinds of baseline architectures trained from scratch on FERV39k (WAR/UAR). </h2></center><table>
			
			<tbody><tr>
				  <td width="400px">
				  <center>
					<a><a href="./FERV39k/result.png"><img class="rounded" src="./FERV39k/result.png" width="750px"></a><br>
				</center>
				</td>
			</tr>
			<tr>
				<td align="center" width="720px">
				  <span style="font-size:14px"><i>
					Results of four kinds of baseline architectures.</i>
				</span>
				</td>
			</tr>
			</tbody></table>

			<br>

			<center><h2> (2) Comparison of cross-scenario results on DL11k, WIS9k, SIA10k, and AI9k of FERV39k on RS50-LSTM. </h2></center><table>
			
			<tbody><tr>
				  <td width="400px">
				  <center>
					  <a><a href="./FERV39k/cross.png"><img class="rounded" src="./FERV39k/cross.png" width="400px"></a><br>
				  </center>
				</td>
			</tr>
			<tr>
				<td align="center" width="720px">
				  <span style="font-size:14px"><i>
					Comparison of cross-scenario results of RS50-LSTM.</i>
				</span>
				</td>
			</tr>
			</tbody></table>


<!--			<br>-->

<!--			<center><h2> (3) How important is temporal information? </h2></center>-->
<!--				(a) Motion features (e.g. optical flows) could capture frame-wise temporal dynamics, leading to better performance of TSN.<br>-->
<!--				(b) Temporal dynamics play an important role in FineGym, and TRN could capture it.<br>-->
<!--				(c) Performance of TSM drops sharply when the number of testing frames is very different from that in training,-->
<!--				while TSN maintains its performance as only temporal average pooling is applied in it.<br><table>-->
<!--				-->
<!--				<tbody><tr>-->
<!--					  <td width="400px">-->
<!--					  <center>-->
<!--						  <a><img class="rounded" src="./FERV39k/temporal_importance.png" width="800px"></a><br>-->
<!--					</center>-->
<!--					</td>-->
<!--				</tr>-->
<!--				<tr>-->
<!--					<td align="center" width="720px">-->
<!--					  <span style="font-size:14px"><i>-->
<!--						(a) Per-class performances of TSN with motion and appearance features in 6 element categories. <br>-->
<!--						(b) Performances of TRN on the set UB-circles using ordered or shuffled testing frames.<br>-->
<!--						(c) Mean-class accuracies of TSM and TSN on Gym99 when trained with 3 frames and tested with more frames.</i>-->
<!--					</span>-->
<!--					</td>-->
<!--				</tr>-->
<!--				</tbody></table>-->

<!--			<br>-->

<!--			<center><h2> (4) Does pre-training on large-scale video datasets help? </h2></center>-->
<!--				On FineGym, pre-training on Kinetics is not always helpful.-->
<!--				One potential reason is the large gaps in terms of temporal patterns between coarse- and fine-grained actions. -->
<!--				<table>-->
<!--				<tbody><tr>-->
<!--					<td width="400px">-->
<!--					<center>-->
<!--						<a><img class="rounded" src="./FERV39k/pretrain.png" width="240px"></a>-->
<!--					</center>-->
<!--					</td>-->
<!--				</tr>-->
<!--				<tr>-->
<!--					<td align="center" width="720px">-->
<!--					<span style="font-size:14px"><i>-->
<!--					Per-class performances of I3D pre-trained on Kinetics and ImageNet in various element categories.</i>-->
<!--					</span>-->
<!--					</td>-->
<!--				</tr>-->
<!--				</tbody></table>-->

<!--			<br>-->
<!--			-->
<!--			<center><h2> (4) Why pose information does not help? </h2></center>-->
<!--				Skeleton-based ST-GCN struggles due to the challenges in skeleton estimation on gymnastics instances.-->
<!--				<table>-->
<!--				<tbody><tr>-->
<!--					<td width="400px">-->
<!--					<center>-->
<!--						<a><img onmouseover="this.src=&#39;./resources/examples/pose_slow.gif&#39;;" onmouseout="this.src=&#39;./resources/examples/pose.gif&#39;;" src="./FERV39k/pose.gif" width="400px"></a>-->
<!--					</center>-->
<!--					</td>-->
<!--				</tr>-->
<!--				<tr>-->
<!--					<td align="center" width="720px">-->
<!--					<span style="font-size:14px"><i>-->
<!--					The results of person detection and pose estimation using <a href="https://github.com/MVIG-SJTU/AlphaPose">AlphaPose</a> for a Vault routine.-->
<!--					It can be seen that detections and pose estimations of the gymnast are missed in multiple frames, especially in frames with intense motion.-->
<!--					These frames are important for fine-grained recognition. (Hover on the GIF for a 0.25x slowdown) </i>-->
<!--					</span>-->
<!--					</td>-->
<!--				</tr>-->
<!--				</tbody></table>-->
<!--					-->
<!--		-->
<!--	      <br><br>-->
			<br>
		  <hr>

		  <center><h1>Download</h1></center><table align="center" width="720px">

<tbody><tr>
<td width="400px">
	<center>
		<a><img class="rounded" src="./FERV39k/data_icon.png" width="50px"></a>
		<br>
</center>
</td>

</tr></tbody></table>

<br>

<center><span style="font-size:16px"><a href="https://space.bilibili.com/401742377">下载链接</a></span><br></center>

<br><br>
<hr>

		  <!-- <center><h1>Download</h1></center><table id="download" align="center" width="720px">
			
			<tbody><tr>
				<td width="300px">
					<center>
						<span style="font-size:24px">Categories</span><br>
						<br>
						<img class="rounded" onmouseover="this.src=&#39;./resources/images/dataset_icon.jpg&#39;;" onmouseout="this.src=&#39;./resources/images/dataset_icon.jpg&#39;;" src="./FERV39k/dataset_icon.jpg" height="150px"><br><br>
						<span style="font-size:16px"><a href="https://kgbmax.github.io/resources/dataset/finegym_glabel_to_Qtree.json">question annotation (json)</a></span><br>
						<span style="font-size:16px"><a href="https://kgbmax.github.io/resources/dataset/set_categories.txt">set-level category list (txt)</a></span><br>
						<span style="font-size:16px"><a href="https://kgbmax.github.io/resources/dataset/gym99_categories.txt">Gym99 category list (txt)</a></span><br>
						<span style="font-size:16px"><a href="https://kgbmax.github.io/resources/dataset/gym288_categories.txt">Gym288 category list (txt)</a></span><br>
						<span style="font-size:16px"><a href="https://kgbmax.github.io/resources/dataset/gym530_categories.txt">Gym530 category list (txt)</a></span><br>
					<span style="font-size:16px"></span>
					</center>
				</td>
				<td width="300px">
					<center>
						<span style="font-size:24px">v1.0</span><br>
						<br>
						<img class="rounded" onmouseover="this.src=&#39;./resources/images/dataset_icon.jpg&#39;;" onmouseout="this.src=&#39;./resources/images/dataset_icon.jpg&#39;;" src="./FERV39k/dataset_icon.jpg" height="150px"><br><br>
						<span style="font-size:16px"><a href="https://kgbmax.github.io/resources/dataset/finegym_annotation_info_v1.0.json">temporal annotation (json)</a></span><br>
						<span style="font-size:16px"><a href="https://kgbmax.github.io/resources/dataset/gym99_train_element_v1.0.txt">Gym99 train split</a></span><br>
						<span style="font-size:16px"><a href="https://kgbmax.github.io/resources/dataset/gym99_val_element.txt">Gym99 val split</a></span><br>
						<span style="font-size:16px"><a href="https://kgbmax.github.io/resources/dataset/gym288_train_element_v1.0.txt">Gym288 train split</a></span><br>
						<span style="font-size:16px"><a href="https://kgbmax.github.io/resources/dataset/gym288_val_element.txt">Gym288 val split</a></span><br>
					<span style="font-size:16px"></span>
					</center>
				</td>
				<td width="300px">
					<center>
						<span style="font-size:24px">v1.1</span><br><br>
						<img class="rounded" onmouseover="this.src=&#39;./resources/images/dataset_icon.jpg&#39;;" onmouseout="this.src=&#39;./resources/images/dataset_icon.jpg&#39;;" src="./FERV39k/dataset_icon.jpg" height="150px"><br><br>
						<span style="font-size:16px"><a href="https://kgbmax.github.io/resources/dataset/finegym_annotation_info_v1.1.json">temporal annotation (json)</a></span><br>
						<span style="font-size:16px"><a href="https://kgbmax.github.io/resources/dataset/gym99_train_element_v1.1.txt">Gym99 train split</a></span><br>
						<span style="font-size:16px"><a href="https://kgbmax.github.io/resources/dataset/gym99_val_element.txt">Gym99 val split (same as v1.0)</a></span><br>
						<span style="font-size:16px"><a href="https://kgbmax.github.io/resources/dataset/gym288_train_element_v1.1.txt">Gym288 train split</a></span><br>
						<span style="font-size:16px"><a href="https://kgbmax.github.io/resources/dataset/gym288_val_element.txt">Gym288 val split (same as v1.0)</a></span><br>
					<span style="font-size:16px"></span>
					</center>
				</td>
				</tr></tbody></table><center><h2> Updates </h2></center>
				[14/02/2022] We published the first version of FERV39K. Check out <a href="https://github.com/SDOlivia/FineGym/">here</a>.<br> -->
<!--				[16/04/2020] We fix a small issue on the naming of the subaction identifier "A_{ZZZZ}_{WWWW}" to avoid ambiguity.-->
<!--				(Thanks <a href="https://kennymckormick.github.io/">Haodong Duan</a> for pointing this out.)<br>-->
<!--				[16/04/2020] We include new subsections to track updates and address FAQs.-->
<br><center><h2> FAQs </h2></center>
Q0: License issue:<br>
A0: The annotations of FERV39k are copyright by us and published under the Creative Commons Attribution-NonCommercial 4.0 International License.<br>
<!--				Q1: Some links are invalid on YouTube. How can I obtain the missing videos?<br>-->
<!--				Q1': I am located in mainland China and I cannot access YouTube. How can I get the dataset?<br>-->
<!--				A1: Please submit a Google form at <a href="https://forms.gle/z8iTwRhdMsfRCNkm7">this link</a>.-->
<!--				We may reach you shortly.<br>-->
<!--				Q2: Is the event-/element-level instance in your dataset cut in integral seconds?<br>-->
<!--				A2: No. All levels of instances (actions and sub-actions) are annotated in <i>exact</i> timestamp (milliseconds) -->
<!--				in a pursuit of frame-level preciseness.-->
<!--				The number in the identifier is derived from integral seconds due to conciseness.-->
<!--				Please refer to the instructions below for details.<br>-->
<!--				Q3: Difference of Mean and Top-1 accuracy in Table 2 &amp; 3?<br>-->
<!--				A3: The Top-K accuracy is the fraction of the instances whose correct label falls in the top-k most confident predictions. In our case we take K=1.<br>-->
<!--				The mean accuracy is the averaged per-class accuracy. To be specific, we calculate the top-1 accuracy of each class i to be A_i.-->
<!--				The mean accuracy is the arithmetic mean of A_{1...N}, i.e. (A_1 + A_2 + ... + A_N)/N, where N is the number of classes.-->
<!--				<center><h2> How to read the temporal annotation files (JSON)? </h2></center> -->
<!--				Below, we show an example entry from the above JSON annotation file:-->
<!--				<pre style="font-family: Courier; font-size:14px">"0LtLS9wROrk": {-->
<!--	"E_002407_002435": {-->
<!--		"event": 4,-->
<!--		"segments": {-->
<!--			"A_0003_0005": {-->
<!--				"stages": 1,-->
<!--				"timestamps": [-->
<!--					[-->
<!--						3.45,-->
<!--						5.64-->
<!--					]-->
<!--				]-->
<!--			},-->
<!--			"A_0006_0008": { ... },-->
<!--			"A_0023_0028": { ... },-->
<!--			...-->
<!--		},-->
<!--		"timestamps": [-->
<!--			[-->
<!--				2407.32,-->
<!--				2435.28-->
<!--			]-->
<!--		]-->
<!--	},-->
<!--	"E_002681_002688": {-->
<!--		"event": 1,-->
<!--		"segments": {-->
<!--			"A_0000_0006": {-->
<!--				"stages": 3,-->
<!--				"timestamps": [-->
<!--					[-->
<!--						0.04,-->
<!--						3.2-->
<!--					],-->
<!--					[-->
<!--						3.2,-->
<!--						4.49-->
<!--					],-->
<!--					[-->
<!--						4.49,-->
<!--						6.57-->
<!--					]-->
<!--				]-->
<!--			}-->
<!--		},-->
<!--		"timestamps": [-->
<!--			[-->
<!--				2681.88,-->
<!--				2688.48-->
<!--			]-->
<!--		]-->
<!--	},-->
<!--	"E_002710_002737": { ... },-->
<!--	...-->
<!--}-->
<!--				</pre><table>-->
<!--				-->
<!--				-->
<!--				</table>-->
<!--				The example shows the annotations related to this video.-->
<!--				First of all, we assign the unique identifier <a style="font-family: Courier">"0LtLS9wROrk"</a> to that video,-->
<!--				which corresponds to the 11-digit YouTube identifier. <br>-->
<!--				It contains all action (event-level) instances, whose names follow the format of <a style="font-family: Courier">"E_{XXXXXX}_{YYYYYY}"</a>.-->
<!--				Here, "E" indicates "Event", and "XXXXXX"/"YYYYYY" indicates the zero-padded starting and ending timestamp (in seconds and truncated to Int).-->
<!--				<br>-->
<!--				Each action instance includes (1) the exact timestamps in the original video ('timestamps', in seconds),-->
<!--				(2) event label ('event'), and-->
<!--				(3) a list of annotated subaction (element-level) instances ('segments').-->
<!--				<br>-->
<!--				The annotated subaction instances follow the format of <a style="font-family: Courier">"A_{ZZZZ}_{WWWW}"</a>.-->
<!--				Here, "A" indicates "subAction", and "ZZZZ"/"WWWW" indicates the zero-padded starting and ending timestamp (in seconds and truncated to Int).-->
<!--				<br>-->
<!--				Ech subaction instance includes (1) the number of stages of this subaction instance ('stages', 3 for Vault and 1 for other events)-->
<!--				(2) the exact timestamps of each stage <i>relative</i> to the starting time of event. ('timestamps', in seconds)-->
<!--				As a result, each subaction instance has a unique identifier <a style="font-family: Courier">"{VIDEO_ID}_E_{XXXXXX}_{YYYYYY}_A_{ZZZZ}_{WWWW}"</a>.-->
<!--				This identifier serves as the instance name in the train/val splits of Gym99 and Gym288.-->
<!--			-->
<!--		   -->

<!--		   <center><h2> How to read the question annotation files (JSON)? </h2></center>-->
<!--			Below, we show an example entry from the above JSON annotation file:-->
<!--			<pre style="font-family: Courier; font-size:14px">"0": {-->
<!--	"BTcode": "1111111",-->
<!--	"questions": [-->
<!--		"round-off onto the springboard?",-->
<!--		"turning entry after round-off (turning in first flight phase)?",-->
<!--		"Facing the coming direction when handstand on vault-->
<!--		(0.5 turn in first flight phase)?",-->
<!--		"Body keep stretched  during salto (stretched salto)?",-->
<!--		"Salto with turn?",-->
<!--		"Facing vault table after landing?",-->
<!--		"Salto with 1.5 turn?"-->
<!--	],-->
<!--	"code": "6.00"-->
<!--},-->
<!--"1": {-->
<!--	"BTcode": "1111110",-->
<!--	"questions": [-->
<!--		"round-off onto the springboard?",-->
<!--		"turning entry after round-off (turning in first flight phase)?",-->
<!--		"Facing the coming direction when handstand on vault-->
<!--		(0.5 turn in first flight phase)?",-->
<!--		"Body keep stretched  during salto (stretched salto)?",-->
<!--		"Salto with turn?",-->
<!--		"Facing vault table after landing?",-->
<!--		"Salto with 1.5 turn?"-->
<!--	],-->
<!--	"code": "5.20"-->
<!--},-->
<!--...-->
<!--			</pre><table>-->
<!--			-->
<!--			</table>-->
<!--			The example shows the questions related to each class.-->
<!--			The identifier corresponds to the label name provided in <a href="https://kgbmax.github.io/resources/dataset/gym530_categories_wset.txt">Gym530 category list</a>.-->
<!--			Each class includes (1) a list of questions that are asked ('quetions'),-->
<!--			(2) a string of binary codes ('BTcode') where 1 refers to 'yes' and 0 refers to 'no',-->
<!--			(3) and original code in the <a href="http://www.fig-gymnastics.com/publicdir/rules/files/en_WAG%20CoP%202017-2020.pdf">official codebook</a>.-->
<!--		-->
	   		   
		 
		 <br><br>
		 <hr>

		 <center><h1>Paper</h1></center><table align="center" width="720px">
			
			   <tbody><tr>
				 <td align="center"><a href="https://space.bilibili.com/401742377"><img class="layered-paper-big" style="height:160px" src="./FERV39k/paper_pdf_thumb.png"></a></td>
				 <td><p style="text-align:justify; text-justify:inter-ideograph;"><span style="font-size:14pt">Yan, Wang et al.<br> FERV39k: A Large-Scale Multi-Scene Dataset <br>for Facial Expression Recognition in Videos. In CVPR, 2022.<br>
				 (<a href="https://space.bilibili.com/401742377">arXiv</a>)
				 <span style="font-size:4pt"><a href="https://kgbmax.github.io/"><br></a>
				 </span>
				 </span></p></td>
				 <td align="center"><a href="https://space.bilibili.com/401742377"><img class="layered-paper-big" style="height:160px" src="./FERV39k/supp_pdf_thumb.png"></a></td>
				 <td><span style="font-size:14pt">
				 (<a href="https://space.bilibili.com/401742377">Additional details/<br>supplementary materials</a>)
				 <span style="font-size:4pt"><a href="https://kgbmax.github.io/"><br></a>
				 </span>
				 </span></td>
			 </tr>
		   </tbody></table>
		 
		 <br><br>
		 <hr>

		  <center><h1>Cite</h1></center><div class="disclaimerbox">
			<!-- <center><h2>How to interpret the results</h2></center> -->

		   <span>
				<!-- <center><span style="font-size:28px"><b>Cite</b></span></center> -->
				<pre style="font-family:Courier; font-size:16px">@inproceedings{wang2022ferv39k,
title={FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression 
	Recognition in Videos},
author={Yan, Wang and Yuxuan, Sun and Yiwen, Huang and Zhongying, Liu and 
	Shuyong, Gao and Weifeng, Ge and Wenqiang, Zhang and Wei, Zhang},
booktitle={IEEE Conference on Computer Vision and Pattern Recognition 
	(CVPR)},
year={2022}
}</pre>
		  </span></div><table align="center" width="720px">
			
		  
  		  </table>

			<br><br>
			<hr>
  
		  	
  		  <table align="center" width="720px">
  			  <tbody><tr>
  	              <td width="400px">
  					<left>
	  		  <center><h1>Acknowledgements</h1></center>
				We sincerely thank the outstanding annotation team for their excellent work（京东众包的名字和链接）.
				The template of this webpage is borrowed from <a href="https://richzhang.github.io/colorization/">Richard Zhang</a><a>.
			</a></left><a>
		</a></td>
			 </tr>
		</tbody></table>

		<br><br>
		<hr>

		<table align="center" width="720px">
			<tbody><tr>
				<td width="400px">
				  <left>
			<center><h1>Contact</h1></center>
			For further questions and suggestions, please contact Yan Wang (<a href="mailto:yanwang19@fudan.edu.cn">yanwang19@fudan.edu.cn</a>).
			
			
		</left>
	</td>
		 </tr>
	</tbody></table>

		<br><br>

<script>
	
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75863369-1', 'auto');
  ga('send', 'pageview');

</script>
              


 
</body></html>
